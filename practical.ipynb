{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanclark/.pyenv/versions/3.8.11/envs/mlmi13/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from Corpora import MovieReviewCorpus\n",
    "from Lexicon import SentimentLexicon\n",
    "from Statistics import SignTest\n",
    "from Classifiers import NaiveBayesText, SVMText\n",
    "from Extensions import SVMDoc2Vec\n",
    "from Constants import PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use corpus pickles\n",
    "use_pickles = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = 'plots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieve corpus\n",
    "# corpus_pickle = \"corpus.pkl\"\n",
    "# if use_pickles and os.path.isfile(corpus_pickle):\n",
    "#     with open(corpus_pickle, 'rb') as f:\n",
    "#         corpus = pickle.load(f)\n",
    "# else:\n",
    "#     corpus=MovieReviewCorpus(stemming=False)\n",
    "#     if os.path.isfile(corpus_pickle):\n",
    "#         os.remove(corpus_pickle)\n",
    "#     with open(corpus_pickle, 'wb') as f:\n",
    "#         pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 1000 POS files to be processed\n",
      "Identified 1000 NEG files to be processed\n",
      "Processing POS files\n",
      "Processing NEG files\n",
      "Identified 1000 POS files to be processed\n",
      "Identified 1000 NEG files to be processed\n",
      "Processing POS files\n",
      "Processing NEG files\n",
      "Identified 1000 POS files to be processed\n",
      "Identified 1000 NEG files to be processed\n",
      "Processing POS files\n",
      "Processing NEG files\n",
      "Identified 1000 POS files to be processed\n",
      "Identified 1000 NEG files to be processed\n",
      "Processing POS files\n",
      "Processing NEG files\n"
     ]
    }
   ],
   "source": [
    "corpus_tag=MovieReviewCorpus(stemming=False)\n",
    "corpus_txt=MovieReviewCorpus(stemming=False,use_txt=True)\n",
    "corpus_txt_upper=MovieReviewCorpus(stemming=False,use_txt=True,lower_case=False)\n",
    "corpus_txt_token=MovieReviewCorpus(stemming=False,use_txt=True,tokenise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sign test for all significance testing\n",
    "signTest=SignTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using sentiment lexicon  ---\")\n",
    "\n",
    "# read in lexicon\n",
    "lexicon=SentimentLexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_df = pd.DataFrame(lexicon.lexicon).T.reset_index().rename(columns={'index': 'word', 0: 'magnitude', 1: 'polarity'})\n",
    "lexicon_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tagged reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "threshold=8\n",
    "\n",
    "lexicon.classify(corpus_tag.reviews,threshold,magnitude=False,weak_polarity=1,strong_polarity=1)\n",
    "token_preds=lexicon.predictions\n",
    "print(f\"token-only results: {lexicon.getAccuracy():.5f}\")\n",
    "\n",
    "lexicon.classify(corpus_tag.reviews,threshold,magnitude=True,weak_polarity=1,strong_polarity=2)\n",
    "magnitude_preds=lexicon.predictions\n",
    "print(f\"magnitude results: {lexicon.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "threshold=8\n",
    "\n",
    "lexicon.classify(corpus_txt.reviews,threshold,magnitude=False,weak_polarity=1,strong_polarity=1)\n",
    "token_preds=lexicon.predictions\n",
    "print(f\"token-only results: {lexicon.getAccuracy():.5f}\")\n",
    "\n",
    "lexicon.classify(corpus_txt.reviews,threshold,magnitude=True,weak_polarity=1,strong_polarity=2)\n",
    "magnitude_preds=lexicon.predictions\n",
    "print(f\"magnitude results: {lexicon.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the tokenised text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on average there are more positive than negative words per review (~7.13 more positive than negative per review)\n",
    "# to take this bias into account will use threshold (roughly the bias itself) to make it harder to classify as positive\n",
    "threshold=8\n",
    "\n",
    "lexicon.classify(corpus_txt_token.reviews,threshold,magnitude=False,weak_polarity=1,strong_polarity=1)\n",
    "token_preds=lexicon.predictions\n",
    "print(f\"token-only results: {lexicon.getAccuracy():.5f}\")\n",
    "\n",
    "lexicon.classify(corpus_txt_token.reviews,threshold,magnitude=True,weak_polarity=1,strong_polarity=2)\n",
    "magnitude_preds=lexicon.predictions\n",
    "print(f\"magnitude results: {lexicon.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_thresholds = np.arange(-20,20)\n",
    "lexicon_pol_gs_df = pd.DataFrame(lexicon_thresholds, columns=['threshold'])\n",
    "def lexicon_pol_gs_func(x):\n",
    "    lexicon.classify(corpus_tag.reviews,x.threshold,magnitude=False,weak_polarity=1,strong_polarity=1)\n",
    "    return lexicon.getAccuracy()\n",
    "lexicon_pol_gs_df['result'] = lexicon_pol_gs_df.apply(lexicon_pol_gs_func,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_pol_gs_df.sort_values('result', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lexicon_pol_gs_df.threshold, lexicon_pol_gs_df.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_thresholds = np.arange(20)\n",
    "lexicon_polarities = np.array(\n",
    "    [[1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [2, 5], [3 ,5]]\n",
    ")\n",
    "lexicon_mag_gs_df = pd.DataFrame(lexicon_thresholds, columns=['threshold']).merge(pd.DataFrame(lexicon_polarities, columns=['weak_polarity', 'strong_polarity']), how='cross')\n",
    "def lexicon_mag_gs_func(x):\n",
    "    lexicon.classify(corpus_tag.reviews,x.threshold,magnitude=True,weak_polarity=x.weak_polarity,strong_polarity=x.strong_polarity)\n",
    "    return lexicon.getAccuracy()\n",
    "lexicon_mag_gs_df['result'] = lexicon_mag_gs_df.apply(lexicon_mag_gs_func,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_mag_gs_df.loc[lexicon_mag_gs_df['threshold'] == 8].sort_values('result', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_mag_gs_df.sort_values('result', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_thresholds = np.arange(-20,20)\n",
    "lexicon_mag_gs_1_2_df = pd.DataFrame(lexicon_thresholds, columns=['threshold'])\n",
    "def lexicon_pol_gs_func(x):\n",
    "    lexicon.classify(corpus_tag.reviews,x.threshold,magnitude=True,weak_polarity=1,strong_polarity=2)\n",
    "    return lexicon.getAccuracy()\n",
    "lexicon_mag_gs_1_2_df['result'] = lexicon_mag_gs_1_2_df.apply(lexicon_pol_gs_func,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "ax.plot(lexicon_pol_gs_df.threshold, lexicon_pol_gs_df.result, label='Polarity')\n",
    "ax.plot(lexicon_mag_gs_1_2_df.threshold, lexicon_mag_gs_1_2_df.result, label='Magnitude')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Prediction Accuracy')\n",
    "ax.legend()\n",
    "fig.savefig(os.path.join(plot_dir, 'lexicon_thresholds.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value=signTest.getSignificance(token_preds,magnitude_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"magnitude lexicon results are {significance} with respect to token-only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tagged reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_tag.train)\n",
    "NB.test(corpus_tag.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_tag.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_tag)\n",
    "# store predictions from classifier\n",
    "nb_tag_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using uppercase text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_upper.train)\n",
    "NB.test(corpus_txt_upper.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_upper.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_upper)\n",
    "# store predictions from classifier\n",
    "nb_txt_upper_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt.train)\n",
    "NB.test(corpus_txt.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt)\n",
    "# store predictions from classifier\n",
    "nb_txt_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tokenised text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "# store predictions from classifier\n",
    "nb_txt_token_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document and Word Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_tag.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_tag.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq = pd.DataFrame(NB.condProb).reset_index().rename(columns={'level_0': 'word'})\n",
    "# nb_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word_freq.sort_values('POS', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word_freq.sort_values('NEG', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_title = lambda x: \"Positive\" if x == \"POS\" else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate(['POS', 'NEG']):\n",
    "    nb_word_freq_top = nb_word_freq.sort_values(sent, ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent]].set_index('word').to_dict()[sent]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent)} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_tag_frequency_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt.train)\n",
    "nb_word_freq = pd.DataFrame(NB.condProb).reset_index().rename(columns={'level_0': 'word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate(['POS', 'NEG']):\n",
    "    nb_word_freq_top = nb_word_freq.sort_values(sent, ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent]].set_index('word').to_dict()[sent]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent.title())} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_txt_frequency_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenised Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token.train)\n",
    "nb_word_freq = pd.DataFrame(NB.condProb).reset_index().rename(columns={'level_0': 'word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate(['POS', 'NEG']):\n",
    "    nb_word_freq_top = nb_word_freq.sort_values(sent, ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent]].set_index('word').to_dict()[sent]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent.title())} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_txt_token_frequency_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_punct = nb_word_freq.loc[~nb_word_freq['word'].isin(PUNCTUATION)]\n",
    "nb_word_freq_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_punct.sort_values('POS', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_punct.sort_values('NEG', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate(['POS', 'NEG']):\n",
    "    nb_word_freq_top = nb_word_freq_punct.sort_values(sent, ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent]].set_index('word').to_dict()[sent]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent.title())} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_frequency_punct_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt_token_punct=MovieReviewCorpus(stemming=False,use_txt=True,allowed_vocab=set(nb_word_freq_punct['word'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token_punct.train)\n",
    "NB.test(corpus_txt_token_punct.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_punct.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_punct)\n",
    "# store predictions from classifier\n",
    "nb_txt_token_punct_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_stop = nb_word_freq.loc[~nb_word_freq['word'].isin(stopwords.words('english'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_stop.sort_values('POS', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_stop.sort_values('NEG', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate(['POS', 'NEG']):\n",
    "    nb_word_freq_top = nb_word_freq_stop.sort_values(sent, ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent]].set_index('word').to_dict()[sent]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent.title())} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_frequency_stop_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt_token_stop=MovieReviewCorpus(stemming=False,use_txt=True,tokenise=True,allowed_vocab=set(nb_word_freq_stop['word'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token_stop.train)\n",
    "NB.test(corpus_txt_token_stop.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_stop.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_stop)\n",
    "# store predictions from classifier\n",
    "nb_txt_token_stop_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_stop_punct = nb_word_freq.loc[~((nb_word_freq['word'].isin(stopwords.words('english'))) | (nb_word_freq['word'].isin(PUNCTUATION)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_stop_punct.sort_values('POS', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_stop_punct.sort_values('NEG', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate(['POS', 'NEG']):\n",
    "    nb_word_freq_top = nb_word_freq_stop_punct.sort_values(sent, ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent]].set_index('word').to_dict()[sent]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent.title())} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_frequency_stop_punct_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt_token_stop_punct=MovieReviewCorpus(stemming=False,use_txt=True,tokenise=True,allowed_vocab=set(nb_word_freq_stop_punct['word'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token_stop_punct.train)\n",
    "NB.test(corpus_txt_token_stop_punct.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_stop_punct.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_stop_punct)\n",
    "# store predictions from classifier\n",
    "nb_txt_token_stop_punct_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude words not in Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_lexicon = lexicon_df.set_index('word').join(nb_word_freq.set_index('word'), how='left').reset_index()\n",
    "nb_word_freq_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_lexicon.loc[nb_word_freq_lexicon['polarity'] == 'positive'].sort_values('POS', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_word_freq_lexicon.loc[nb_word_freq_lexicon['polarity'] == 'negative'].sort_values('NEG', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "for i, sent in enumerate([('POS', 'positive'), ('NEG', 'negative')]):\n",
    "    nb_word_freq_top = nb_word_freq_lexicon.loc[nb_word_freq_lexicon['polarity'] == sent[1]].sort_values(sent[0], ascending=False)\n",
    "    nb_word_freq_top_dict = nb_word_freq_top[['word', sent[0]]].set_index('word').to_dict()[sent[0]]\n",
    "    wordcloud = WordCloud(background_color='white', collocations=False).generate_from_frequencies(nb_word_freq_top_dict)\n",
    "    ax[i].imshow(wordcloud)\n",
    "    ax[i].set_title(f'Class: {cloud_title(sent)} Reviews')\n",
    "    ax[i].set_xticks([0])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_frequency_lexicon_wordcloud.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt_token_lexicon=MovieReviewCorpus(stemming=False,use_txt=True,tokenise=True,allowed_vocab=set(lexicon_df['word'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token_lexicon.train)\n",
    "NB.test(corpus_txt_token_lexicon.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_lexicon.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_lexicon)\n",
    "# store predictions from classifier\n",
    "nb_txt_token_lexicon_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if limiting the vocabulary to the lexicon significantly improves results\n",
    "p_value=signTest.getSignificance(nb_txt_token_non_smoothed_preds,nb_txt_token_lexicon_non_smoothed_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results limiting the vocabulary to the lexicon are {significance} with respect to not doing so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only allow words that are in the lexicon with positive or negative polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_txt_token_lexicon_pol=MovieReviewCorpus(stemming=False,use_txt=True,tokenise=True,allowed_vocab=set(lexicon_df.loc[lexicon_df['polarity'].isin(['negative', 'positive'])]['word'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using Naive Bayes on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token_lexicon_pol.train)\n",
    "NB.test(corpus_txt_token_lexicon_pol.train, verbose=False)\n",
    "print(f\"Training accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_lexicon_pol.test, verbose=False)\n",
    "print(f\"Test Accuracy without smoothing: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB without smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_lexicon_pol)\n",
    "# store predictions from classifier\n",
    "nb_txt_token_lexicon_pol_non_smoothed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tagged reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_tag.train)\n",
    "NB.test(corpus_tag.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_tag.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_tag)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using uppercase text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_upper.train)\n",
    "NB.test(corpus_txt_upper.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_upper.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_upper)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt.train)\n",
    "NB.test(corpus_txt.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tokenised text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "nb_smoothed_txt_token_preds = NB.predictions\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tokenised stop word text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token_stop.train)\n",
    "NB.test(corpus_txt_token_stop.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_stop.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_stop)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tokenised stop punct text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token_stop_punct.train)\n",
    "NB.test(corpus_txt_token_stop_punct.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_stop_punct.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_stop_punct)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tokenised text lexicon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token_lexicon.train)\n",
    "NB.test(corpus_txt_token_lexicon.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_lexicon)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the tokenised pos/neg text lexicon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 1\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token_lexicon_pol.train)\n",
    "NB.test(corpus_txt_token_lexicon_pol.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token_lexicon_pol)\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.0\n",
    "Moved this part up so that I could use the predictions from cross-evaluation on Q2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# # using previous instantiated object\n",
    "# NB.crossValidate(corpus)\n",
    "# print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_extract_frequencies(cond_prod):\n",
    "    df = pd.DataFrame(cond_prod).reset_index().rename(columns={'level_0': 'word'}).sort_values('word')\n",
    "    return np.array(df['word']), np.array(df['POS']), np.array(df['NEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_laplace_smoothing_vals_large = np.arange(0.1,10,0.1)\n",
    "nb_laplace_smoothing_test_accuracies = np.zeros_like(nb_laplace_smoothing_vals_large)\n",
    "for i, laplace_smoothing in tqdm(enumerate(nb_laplace_smoothing_vals_large)):\n",
    "    NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False,laplacian_k=laplace_smoothing)\n",
    "    NB.train(corpus_txt_token.train)\n",
    "    NB.test(corpus_txt_token.test, verbose=False)\n",
    "    words, _, _ = nb_extract_frequencies(NB.condProb)\n",
    "    nb_laplace_smoothing_test_accuracies[i] = NB.getAccuracy()\n",
    "nb_laplace_smoothing_test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_laplace_smoothing_vals_large[np.argmax(nb_laplace_smoothing_test_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_laplace_smoothing_vals = [0, 0.1, 1, 10]\n",
    "nb_laplace_smoothing_pos_frequencies = np.zeros((len(nb_laplace_smoothing_vals), 45974))\n",
    "nb_laplace_smoothing_neg_frequencies = np.zeros((len(nb_laplace_smoothing_vals), 45974))\n",
    "for i, laplace_smoothing in tqdm(enumerate(nb_laplace_smoothing_vals)):\n",
    "    NB=NaiveBayesText(smoothing=laplace_smoothing > 0,bigrams=False,trigrams=False,discard_closed_class=False,laplacian_k=laplace_smoothing)\n",
    "    NB.train(corpus_txt_token.train)\n",
    "    NB.test(corpus_txt_token.test, verbose=False)\n",
    "    words, pos_freq, neg_freq = nb_extract_frequencies(NB.condProb)\n",
    "\n",
    "    nb_laplace_smoothing_pos_frequencies[i,:] = pos_freq\n",
    "    nb_laplace_smoothing_neg_frequencies[i,:] = neg_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(30,10))\n",
    "\n",
    "nb_laplace_smoothing_pos_frequencies_argsort = np.argsort(-nb_laplace_smoothing_pos_frequencies[0,:])\n",
    "\n",
    "ax[0].plot(nb_laplace_smoothing_pos_frequencies[:, nb_laplace_smoothing_pos_frequencies_argsort[:100]].T)\n",
    "ax[0].set_title(\"Top 100 Words\")\n",
    "ax[0].set_xlabel(\"Word\")\n",
    "ax[0].set_ylabel(\"Smoothed Probability\")\n",
    "ax[0].set_xticks(np.arange(0,100,10))\n",
    "ax[0].set_xlim(-1,100)\n",
    "ax[0].set_xticklabels(words[nb_laplace_smoothing_pos_frequencies_argsort[0:100:10]], rotation=45)\n",
    "ax[0].legend(['Maximum Likelihood', r'$\\kappa=0.1$', r'$\\kappa=1$', r'$\\kappa=10$'], loc='center right')\n",
    "\n",
    "ax[1].plot(nb_laplace_smoothing_pos_frequencies[:, nb_laplace_smoothing_pos_frequencies_argsort[-100:]].T)\n",
    "ax[1].set_title(\"Bottom 100 Words\")\n",
    "ax[1].set_xlabel(\"Word\")\n",
    "ax[1].set_ylabel(\"Smoothed Probability\")\n",
    "ax[1].set_xticks(np.arange(0,100,10))\n",
    "ax[1].set_xlim(-1,100)\n",
    "ax[1].set_xticklabels(words[nb_laplace_smoothing_pos_frequencies_argsort[-100::10]], rotation=45)\n",
    "ax[1].legend(['Maximum Likelihood', r'$\\kappa=0.1$', r'$\\kappa=1$', r'$\\kappa=10$'], loc='center right')\n",
    "\n",
    "ax[2].plot(nb_laplace_smoothing_vals_large, nb_laplace_smoothing_test_accuracies)\n",
    "ax[2].set_title(\"Test Accuracy\")\n",
    "ax[2].set_xlabel(r'Laplace Smoothing Factor, $\\kappa$')\n",
    "ax[2].set_ylabel(\"Accuracy\")\n",
    "ax[2].set_xlim(0,10)\n",
    "\n",
    "fig.savefig(os.path.join(plot_dir, 'nb_smoothed_word_probabilities.jpeg'), pad_inches=0.2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing\n",
    "laplacian_k = 3.6\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB and smoothing using 10-fold cross-evaluation ---\")\n",
    "# using previous instantiated object\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "# saving this for use later\n",
    "num_non_stemmed_features=len(NB.vocabulary)\n",
    "# using cross-eval for smoothed predictions from now on\n",
    "nb_smoothed_txt_token_optimised_preds=NB.predictions\n",
    "print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if hyperparameter tuning significantly improves results\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_preds,nb_smoothed_txt_token_optimised_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using tuned hyperparameters are {significance} with respect to defaults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at other corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian_k_vals = [0.1, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_smoothed_test_accuracies = np.zeros(len(laplacian_k_vals))\n",
    "# for i, laplacian_k in enumerate(laplacian_k_vals):\n",
    "#     NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "#     NB.train(corpus_punct.train)\n",
    "#     NB.test(corpus_punct.test, verbose=False)\n",
    "#     nb_smoothed_test_accuracies[i] = NB.getAccuracy()\n",
    "\n",
    "# laplacian_k = laplacian_k_vals[np.argmax(nb_smoothed_test_accuracies)]\n",
    "\n",
    "# NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "# NB.train(corpus_punct.train)\n",
    "# NB.test(corpus_punct.train, verbose=False)\n",
    "# print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.test(corpus_punct.test, verbose=False)\n",
    "# print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.crossValidate(corpus_punct)\n",
    "# print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_smoothed_test_accuracies = np.zeros(len(laplacian_k_vals))\n",
    "# for i, laplacian_k in enumerate(laplacian_k_vals):\n",
    "#     NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "#     NB.train(corpus_stop.train)\n",
    "#     NB.test(corpus_stop.test, verbose=False)\n",
    "#     nb_smoothed_test_accuracies[i] = NB.getAccuracy()\n",
    "\n",
    "# laplacian_k = laplacian_k_vals[np.argmax(nb_smoothed_test_accuracies)]\n",
    "\n",
    "# NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "# NB.train(corpus_stop.train)\n",
    "# NB.test(corpus_stop.train, verbose=False)\n",
    "# print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.test(corpus_stop.test, verbose=False)\n",
    "# print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.crossValidate(corpus_stop)\n",
    "# print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_smoothed_test_accuracies = np.zeros(len(laplacian_k_vals))\n",
    "# for i, laplacian_k in enumerate(laplacian_k_vals):\n",
    "#     NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "#     NB.train(corpus_stop_punct.train)\n",
    "#     NB.test(corpus_stop_punct.test, verbose=False)\n",
    "#     nb_smoothed_test_accuracies[i] = NB.getAccuracy()\n",
    "\n",
    "# laplacian_k = laplacian_k_vals[np.argmax(nb_smoothed_test_accuracies)]\n",
    "\n",
    "# NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "# NB.train(corpus_stop_punct.train)\n",
    "# NB.test(corpus_stop_punct.train, verbose=False)\n",
    "# print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.test(corpus_stop_punct.test, verbose=False)\n",
    "# print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.crossValidate(corpus_stop_punct)\n",
    "# print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_smoothed_test_accuracies = np.zeros(len(laplacian_k_vals))\n",
    "# for i, laplacian_k in enumerate(laplacian_k_vals):\n",
    "#     NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "#     NB.train(corpus_lexicon.train)\n",
    "#     NB.test(corpus_lexicon.test, verbose=False)\n",
    "#     nb_smoothed_test_accuracies[i] = NB.getAccuracy()\n",
    "\n",
    "# laplacian_k = laplacian_k_vals[np.argmax(nb_smoothed_test_accuracies)]\n",
    "\n",
    "# NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False, laplacian_k=laplacian_k)\n",
    "# NB.train(corpus_lexicon.train)\n",
    "# NB.test(corpus_lexicon.train, verbose=False)\n",
    "# print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.test(corpus_lexicon.test, verbose=False)\n",
    "# print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Number of ties: {NB.ties}\")\n",
    "# NB.crossValidate(corpus_lexicon)\n",
    "# print(f\"Accuracy with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "# print(f\"Std. Dev with laplacian {laplacian_k}: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if smoothing significantly improves results\n",
    "p_value=signTest.getSignificance(nb_txt_token_non_smoothed_preds,nb_smoothed_txt_token_optimised_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing are {significance} with respect to no smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- stemming corpus ---\")\n",
    "# retrieve corpus with tokenized text and stemming (using porter)\n",
    "stemmed_corpus_pickle = \"corpus_stem.pkl\"\n",
    "if use_pickles and os.path.isfile(stemmed_corpus_pickle):\n",
    "    with open(stemmed_corpus_pickle, 'rb') as f:\n",
    "        corpus_txt_token_stemmed = pickle.load(f)\n",
    "else:\n",
    "    corpus_txt_token_stemmed=MovieReviewCorpus(stemming=True,use_txt=True,tokenise=True)\n",
    "    with open(stemmed_corpus_pickle, 'wb') as f:\n",
    "        pickle.dump(corpus_txt_token_stemmed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tag_stemmed = MovieReviewCorpus(stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_k = 3.6\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False,laplacian_k=laplacian_k)\n",
    "NB.train(corpus_txt_token_stemmed.train)\n",
    "NB.test(corpus_txt_token_stemmed.train, verbose=False)\n",
    "print(f\"Training Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")\n",
    "NB.test(corpus_txt_token_stemmed.test, verbose=False)\n",
    "print(f\"Test Accuracy using smoothing with laplacian {laplacian_k}: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Number of ties: {NB.ties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews with NB, smoothing and stemming using 10-fold cross-evaluation ---\")\n",
    "NB.crossValidate(corpus_txt_token_stemmed)\n",
    "# store predictions from classifier\n",
    "nb_smoothed_txt_token_stemmed_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\")\n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if stemming significantly improves results on smoothed NB\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_optimised_preds,nb_smoothed_txt_token_stemmed_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using stemming are {significance} with respect to no stemming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "\n",
    "NB.extractVocabulary(corpus_txt_token.reviews)\n",
    "print(f\"features before stemming (txt - complete): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_txt_token.train)\n",
    "print(f\"features before stemming (txt - train): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_txt_token.test)\n",
    "print(f\"features before stemming (txt - test): {len(NB.vocabulary)}\")\n",
    "num_unigrams_features = len(NB.vocabulary)\n",
    "\n",
    "NB.extractVocabulary(corpus_txt_token_stemmed.reviews)\n",
    "print(f\"features after stemming (txt - complete): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_txt_token_stemmed.train)\n",
    "print(f\"features after stemming (txt - train): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_txt_token_stemmed.test)\n",
    "print(f\"features after stemming (txt - test): {len(NB.vocabulary)}\")\n",
    "\n",
    "NB.extractVocabulary(corpus_tag.reviews)\n",
    "print(f\"features before stemming (tag - complete): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_tag.train)\n",
    "print(f\"features before stemming (tag - train): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_tag.test)\n",
    "print(f\"features before stemming (tag - test): {len(NB.vocabulary)}\")\n",
    "\n",
    "NB.extractVocabulary(corpus_tag_stemmed.reviews)\n",
    "print(f\"features after stemming (tag - complete): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_tag_stemmed.train)\n",
    "print(f\"features after stemming (tag - train): {len(NB.vocabulary)}\")\n",
    "NB.extractVocabulary(corpus_tag_stemmed.test)\n",
    "print(f\"features after stemming (tag - test): {len(NB.vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing and unigrams and bigrams\n",
    "print(\"--- classifying reviews using Naive Bayes using smoothing with unigrams and bigrams on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "print(f\"Training accuracy using smoothing and bigrams: {NB.getAccuracy():.5f}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "num_bigrams_features=len(NB.vocabulary)\n",
    "print(f\"Test accuracy using smoothing and bigrams: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate model using smoothing and unigrams and bigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and unigrams and bigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "nb_smoothed_txt_token_bigram_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if unigrams and bigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_preds,nb_smoothed_txt_token_bigram_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and unigrams and bigrams are {significance} with respect to smoothing only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing and unigrams, bigrams and trigrams\n",
    "print(\"--- classifying reviews using Naive Bayes using smoothing with unigrams, bigrams and trigrams on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=True,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "num_bigrams_and_trigrams_features=len(NB.vocabulary)\n",
    "print(f\"Training accuracy using smoothing and unigrams, bigrams and trigrams: {NB.getAccuracy():.5f}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Testing accuracy using smoothing and unigrams, bigrams and trigrams: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate model using smoothing and bigrams and trigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and bigrams and trigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,bigrams=True,trigrams=True,discard_closed_class=False)\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "nb_smoothed_txt_token_bigram_and_trigram_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if unigrams, bigrams and trigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_preds,nb_smoothed_txt_token_bigram_and_trigram_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and bigrams and trigrams are {significance} with respect to smoothing only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if unigrams, bigrams and trigrams significantly improves results on unigrams and bigrams NB only\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_preds,nb_smoothed_txt_token_bigram_and_trigram_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and bigrams and trigrams are {significance} with respect to bigrams only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing and bigrams only\n",
    "print(\"--- classifying reviews using Naive Bayes using smoothing with bigrams on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=True,unigrams=False,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "num_bigrams_only_features=len(NB.vocabulary)\n",
    "print(f\"Training accuracy using smoothing and bigrams only: {NB.getAccuracy():.5f}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test accuracy using smoothing and bigrams only: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate model using smoothing and bigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and bigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,unigrams=False,bigrams=True,trigrams=False,discard_closed_class=False)\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "# saving this for use later\n",
    "num_bigrams_only_features=len(NB.vocabulary)\n",
    "# store predictions from classifier\n",
    "nb_smoothed_txt_token_bigram_only_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if bigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_preds,nb_smoothed_txt_token_bigram_only_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and bigrams are {significance} with respect to smoothing only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use smoothing and trigrams only\n",
    "print(\"--- classifying reviews using Naive Bayes using smoothing with trigrams on held-out test set ---\")\n",
    "NB=NaiveBayesText(smoothing=True,unigrams=False,bigrams=False,trigrams=True,discard_closed_class=False)\n",
    "NB.train(corpus_txt_token.train)\n",
    "NB.test(corpus_txt_token.train, verbose=False)\n",
    "num_trigrams_only_features=len(NB.vocabulary)\n",
    "print(f\"Training accuracy using smoothing and trigrams only: {NB.getAccuracy():.5f}\")\n",
    "NB.test(corpus_txt_token.test, verbose=False)\n",
    "print(f\"Test accuracy using smoothing and trigrams only: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate model using smoothing and trigrams\n",
    "print(\"--- cross-validating naive bayes using smoothing and trigrams ---\")\n",
    "NB=NaiveBayesText(smoothing=True,unigrams=False,bigrams=False,trigrams=True,discard_closed_class=False)\n",
    "NB.crossValidate(corpus_txt_token)\n",
    "# store predictions from classifier\n",
    "nb_smoothed_txt_token_trigram_only_preds=NB.predictions\n",
    "print(f\"Accuracy: {NB.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {NB.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if trigrams significantly improves results on smoothed NB only\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_optimised_preds,nb_smoothed_txt_token_trigram_only_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using smoothing and trigrams are {significance} with respect to smoothing only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"features with unigrams: {num_unigrams_features}\")\n",
    "print(f\"features with unigrams and bigrams: {num_bigrams_features}\")\n",
    "print(f\"features with unigrams, bigrams and trigrams: {num_bigrams_and_trigrams_features}\")\n",
    "print(f\"features with bigrams: {num_bigrams_only_features}\")\n",
    "print(f\"features with trigrams: {num_trigrams_only_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([num_unigrams_features, num_bigrams_only_features, num_trigrams_only_features])\n",
    "n_bi, n_tri = np.log(num_bigrams_only_features)/np.log(num_unigrams_features), np.log(num_trigrams_only_features)/np.log(num_unigrams_features)\n",
    "plt.plot([num_unigrams_features, num_unigrams_features**n_bi, num_unigrams_features**n_tri], label='Fit')\n",
    "plt.plot([num_unigrams_features, num_bigrams_features, num_bigrams_and_trigrams_features])\n",
    "plt.plot([num_unigrams_features, num_unigrams_features+num_unigrams_features**n_bi, num_unigrams_features+num_unigrams_features**n_bi+num_unigrams_features**n_tri], label='Fit')\n",
    "plt.legend()\n",
    "n_bi, n_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 and 6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using SVM on held-out test set ---\")\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=False,idf=False)\n",
    "SVM.train(corpus_txt_token.train)\n",
    "SVM.test(corpus_txt_token.train)\n",
    "print(f\"Training accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_txt_token.test)\n",
    "print(f\"Test accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_txt_token,verbose=False)\n",
    "svm_og_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using SVM on held-out test set ---\")\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=True,idf=False)\n",
    "SVM.train(corpus_txt_token.train)\n",
    "SVM.test(corpus_txt_token.train)\n",
    "print(f\"Training accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_txt_token.test)\n",
    "print(f\"Test accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_txt_token,verbose=False)\n",
    "svm_tf_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if using term frequency significantly improves results on words counts\n",
    "p_value=signTest.getSignificance(svm_og_preds,svm_tf_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using tf {significance} with respect to word counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using SVM on held-out test set ---\n",
      "Training accuracy with SVM using unigrams: 0.99944\n",
      "Test accuracy with SVM using unigrams: 0.84500\n"
     ]
    }
   ],
   "source": [
    "print(\"--- classifying reviews using SVM on held-out test set ---\")\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=True,idf=True)\n",
    "SVM.train(corpus_txt_token.train)\n",
    "SVM.test(corpus_txt_token.train)\n",
    "print(f\"Training accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_txt_token.test)\n",
    "print(f\"Test accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\n",
      "Accuracy: 0.83250\n",
      "Std. Dev: 0.01365\n"
     ]
    }
   ],
   "source": [
    "print(\"--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_txt_token,verbose=False)\n",
    "# store predictions from classifier\n",
    "svm_tf_idf_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if using tf-idf significantly improves results on words counts\n",
    "p_value=signTest.getSignificance(svm_og_preds,svm_tf_idf_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using tf-idf {significance} with respect to word counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if using tf-idf significantly improves results on tf\n",
    "p_value=signTest.getSignificance(svm_tf_preds,svm_tf_idf_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using tf-idf {significance} with respect to tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs_params = {\n",
    "    \"C\": np.arange(0.2, 10.2, 0.4),\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "}\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=True,idf=True)\n",
    "svc_gs = SVM.train(corpus_txt_token.train, grid_search_params=svm_gs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs.best_params_, svc_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using SVM on held-out test set ---\n",
      "Training accuracy with SVM using unigrams: 0.99944\n",
      "Test accuracy with SVM using unigrams: 0.89000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- classifying reviews using SVM on held-out test set ---\")\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=True,idf=True,C=1.8,kernel='linear')\n",
    "SVM.train(corpus_txt_token.train)\n",
    "SVM.test(corpus_txt_token.train)\n",
    "print(f\"Training accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_txt_token.test)\n",
    "print(f\"Test accuracy with SVM using unigrams: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\n",
      "Accuracy: 0.86100\n",
      "Std. Dev: 0.01997\n"
     ]
    }
   ],
   "source": [
    "print(\"--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_txt_token,verbose=False)\n",
    "# store predictions from classifier\n",
    "svm_tf_idf_opt_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results using optimised SVC hyperparameters not significant with respect to defaults\n"
     ]
    }
   ],
   "source": [
    "# see if using optimised hyperameters significantly improves default results\n",
    "p_value=signTest.getSignificance(svm_tf_idf_preds, svm_tf_idf_opt_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using optimised SVC hyperparameters {significance} with respect to defaults\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if SVM significantly improves results on smoothed NB\n",
    "p_value=signTest.getSignificance(nb_smoothed_txt_token_preds,svm_tf_idf_opt_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using SVM {significance} with respect to smoothed NB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=False,tf=True,idf=True,C=1.8,kernel='linear')\n",
    "SVM=SVMText(bigrams=True,trigrams=False,discard_closed_class=False,tf=True,idf=True)\n",
    "SVM.train(corpus_txt_token.train)\n",
    "SVM.test(corpus_txt_token.train)\n",
    "print(f\"Training accuracy with SVM using additional bigrams: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_txt_token.test)\n",
    "print(f\"Test accuracy with SVM using additional bigrams: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_txt_token,verbose=False)\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM=SVMText(bigrams=True,trigrams=True,discard_closed_class=False,tf=True,idf=True,C=1.8,kernel='linear')\n",
    "SVM=SVMText(bigrams=True,trigrams=True,discard_closed_class=False,tf=True,idf=True)\n",
    "SVM.train(corpus_txt_token.train)\n",
    "SVM.test(corpus_txt_token.train)\n",
    "print(f\"Training accuracy with SVM using additional bigrams and trigrams: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_txt_token.test)\n",
    "print(f\"Teat accuracy with SVM using additional bigrams and trigrams: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using SVM with unigrams and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_txt_token,verbose=False)\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- adding in POS information to corpus ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- pos corpus ---\")\n",
    "# # retrieve corpus with tokenized text and pos\n",
    "# pos_corpus_pickle = \"corpus_pos.pkl\"\n",
    "# if use_pickles and os.path.isfile(pos_corpus_pickle):\n",
    "#     with open(pos_corpus_pickle, 'rb') as f:\n",
    "#         pos_corpus = pickle.load(f)\n",
    "# else:\n",
    "#     pos_corpus=MovieReviewCorpus(pos=True)\n",
    "#     with open(pos_corpus_pickle, 'wb') as f:\n",
    "#         pickle.dump(pos_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_corpus=MovieReviewCorpus(pos=True)\n",
    "corpus_tag_pos=MovieReviewCorpus(pos=True,stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- training nb on word+pos features ----\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_tag_pos.train)\n",
    "NB.test(corpus_tag_pos.test, verbose=False)\n",
    "print(f\"Accuracy using NB on unigrams without smoothing and with POS: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=False)\n",
    "NB.train(corpus_tag_pos.train)\n",
    "NB.test(corpus_tag_pos.test, verbose=False)\n",
    "print(f\"Accuracy using NB on unigrams with smoothing and POS: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- training svm on word+pos features ----\")\n",
    "# SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=True,idf=True,C=1.8,kernel='linear')\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=False,tf=True,idf=True)\n",
    "SVM.train(corpus_tag_pos.train)\n",
    "SVM.test(corpus_tag_pos.train)\n",
    "print(f\"Training accuracy with SVM with POS: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_tag_pos.test)\n",
    "print(f\"Test accuracy with SVM with POS: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using svm on word+pos and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_tag_pos)\n",
    "# store predictions from classifier\n",
    "svm_pos_preds=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if POS significantly improves results on SVM only\n",
    "p_value=signTest.getSignificance(svm_tf_idf_preds,svm_pos_preds)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results using POS tags {significance} with respect to SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discard Closed Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- training nb discarding closed-class words ---\")\n",
    "NB=NaiveBayesText(smoothing=False,bigrams=False,trigrams=False,discard_closed_class=True)\n",
    "NB.train(corpus_tag_pos.train)\n",
    "NB.test(corpus_tag_pos.train, verbose=False)\n",
    "print(f\"Training accuracy using NB without smoothing and discarding closed-class words: {NB.getAccuracy():.5f}\")\n",
    "NB.test(corpus_tag_pos.test, verbose=False)\n",
    "print(f\"Test accuracy using NB without smoothing and discarding closed-class words: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=NaiveBayesText(smoothing=True,bigrams=False,trigrams=False,discard_closed_class=True)\n",
    "NB.train(corpus_tag_pos.train)\n",
    "NB.test(corpus_tag_pos.train, verbose=False)\n",
    "print(f\"Training accuracy using NB with smoothing and discarding closed-class words: {NB.getAccuracy():.5f}\")\n",
    "NB.test(corpus_tag_pos.test, verbose=False)\n",
    "print(f\"Test accuracy using NB with smoothing and discarding closed-class words: {NB.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- training svm discarding closed-class words ---\")\n",
    "# SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=True,C=1.8,kernel='linear')\n",
    "SVM=SVMText(bigrams=False,trigrams=False,discard_closed_class=True)\n",
    "SVM.train(corpus_tag_pos.train)\n",
    "SVM.test(corpus_tag_pos.train)\n",
    "print(f\"Training accuracy with SVM discarding closed-class word: {SVM.getAccuracy():.5f}\")\n",
    "SVM.test(corpus_tag_pos.test)\n",
    "print(f\"Test accuracy with SVM discarding closed-class word: {SVM.getAccuracy():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- classifying reviews using svm discarding closed-class words and 10-fold cross-eval ---\")\n",
    "SVM.crossValidate(corpus_tag_pos)\n",
    "# store predictions from classifier\n",
    "svm_closed_class=SVM.predictions\n",
    "print(f\"Accuracy: {SVM.getAccuracy():.5f}\") \n",
    "print(f\"Std. Dev: {SVM.getStdDeviation():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if discarding closed-class words significantly improves results on SVM only\n",
    "p_value=signTest.getSignificance(svm_tf_idf_preds,svm_closed_class)\n",
    "significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"results discardig closed-class words {significance} with respect to SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- using document embeddings ---\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0fa47442b815d01bf03fa5e1a4dae12be90d6f5e4d1707ce225297341a1eb48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('mlmi13': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
