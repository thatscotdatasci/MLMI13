{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanclark/.pyenv/versions/3.8.11/envs/mlmi13/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from Doc2Vec import Doc2Vec, SVMSklearn, TSNESklearn, GensimSVMSklearn\n",
    "from Constants import SENTIMENTS, TRAINING_DATA, TESTING_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Original review data\n",
    "#####################\n",
    "base_dir = os.path.join('data', 'reviews')\n",
    "pos_dir = os.path.join(base_dir, 'POS')\n",
    "neg_dir = os.path.join(base_dir, 'NEG')\n",
    "\n",
    "training_pos_files = glob(os.path.join(pos_dir, 'cv[0-8]*.txt'))\n",
    "training_neg_files = glob(os.path.join(neg_dir, 'cv[0-8]*.txt'))\n",
    "\n",
    "testing_pos_files = glob(os.path.join(pos_dir, 'cv9*.txt'))\n",
    "testing_neg_files = glob(os.path.join(neg_dir, 'cv9*.txt'))\n",
    "\n",
    "d2v_training_files = [\n",
    "    *training_pos_files,\n",
    "    *training_neg_files,\n",
    "    *testing_pos_files,\n",
    "    *testing_neg_files\n",
    "]\n",
    "d2v_testing_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########\n",
    "# # IMDB data\n",
    "# ###########\n",
    "# base_dir = 'imdb'\n",
    "# pos_dir = 'pos'\n",
    "# neg_dir = 'neg'\n",
    "# unsup_dir = 'unsup'\n",
    "\n",
    "# train_dir = os.path.join(base_dir, 'train')\n",
    "# train_pos_dir = os.path.join(train_dir, pos_dir)\n",
    "# train_neg_dir = os.path.join(train_dir, neg_dir)\n",
    "\n",
    "# test_dir = os.path.join(base_dir, 'train')\n",
    "# test_pos_dir = os.path.join(test_dir, pos_dir)\n",
    "# test_neg_dir = os.path.join(test_dir, neg_dir)\n",
    "\n",
    "# training_pos_files = glob(os.path.join(train_pos_dir, '*.txt'))\n",
    "# training_neg_files = glob(os.path.join(train_neg_dir, '*.txt'))\n",
    "# testing_pos_files = glob(os.path.join(test_pos_dir, '*.txt'))\n",
    "# testing_neg_files = glob(os.path.join(test_neg_dir, '*.txt'))\n",
    "\n",
    "# unsup_files = glob(os.path.join(base_dir, train_dir, unsup_dir, '*.txt'))\n",
    "\n",
    "# d2v_training_files = [\n",
    "#     *training_pos_files,\n",
    "#     *training_neg_files,\n",
    "#     *unsup_files\n",
    "# ]\n",
    "# d2v_testing_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([\n",
    "    *[SENTIMENTS.pos.review_label]*len(training_pos_files),\n",
    "    *[SENTIMENTS.neg.review_label]*len(training_neg_files)\n",
    "])\n",
    "y_test = np.array([\n",
    "    *[SENTIMENTS.pos.review_label]*len(testing_pos_files),\n",
    "    *[SENTIMENTS.neg.review_label]*len(testing_neg_files)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn = GensimSVMSklearn(\n",
    "    d2v_training_files=d2v_training_files,\n",
    "    d2v_epochs=100,\n",
    "    d2v_infer_epochs=50,\n",
    "    d2v_min_count=5,\n",
    "    d2v_vector_size=50,\n",
    "    d2v_window=5,\n",
    "    d2v_dm=0,\n",
    "    d2v_dm_concat=0,\n",
    "    d2v_dbow_words=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.train([*training_pos_files, *training_neg_files], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_X_train = gensim_sklearn.pipeline.named_steps['doc2vec'].transform([*training_pos_files, *training_neg_files])\n",
    "pipeline_X_test = gensim_sklearn.pipeline.named_steps['doc2vec'].transform([*testing_pos_files, *testing_neg_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.test([*training_pos_files, *training_neg_files], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.test([*testing_pos_files, *testing_neg_files], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.cross_validate([*training_pos_files, *training_neg_files], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_params = {\n",
    "#     'doc2vec__epochs': (100,),\n",
    "#     'doc2vec__infer_epochs': (50,),\n",
    "#     'doc2vec__vector_size': (50,), \n",
    "#     'doc2vec__dm': (0,),\n",
    "#     'doc2vec__dm_concat': (0,),\n",
    "#     'doc2vec__dbow_words': (1,),\n",
    "#     'doc2vec__window': (5,10,15,20),\n",
    "#     'doc2vec__min_count': (5,)\n",
    "# }\n",
    "        \n",
    "# gensim_sklearn.grid_search([*training_pos_files, *training_neg_files], y_train, gs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(gensim_sklearn.gs.cv_results_).to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('.pkl', 'wb') as f:\n",
    "    # pickle.dump(gensim_sklearn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Doc2Vec\n",
    "#########\n",
    "\n",
    "use_d2v_pickle = False\n",
    "d2v_pickle_name = 'doc2vec_model.pkl'\n",
    "\n",
    "if use_d2v_pickle and os.path.isfile(d2v_pickle_name):\n",
    "    logger.info('Loading pickled d2v model')\n",
    "    with open(d2v_pickle_name, 'rb') as f:\n",
    "        d2v = pickle.load(f)\n",
    "else:\n",
    "    d2v = Doc2Vec(vector_size=50, epochs=40)\n",
    "\n",
    "    logger.info('Loading data')\n",
    "    d2v.load_data(training_files=d2v_training_files, testing_files=d2v_testing_files)\n",
    "\n",
    "    logger.info('Training doc2vec')\n",
    "    d2v.train()\n",
    "\n",
    "    with open(d2v_pickle_name, 'wb') as f:\n",
    "        pickle.dump(d2v, f)\n",
    "\n",
    "# logger.info('Testing doc2vec on the training data')\n",
    "# ranks_count, errors = d2v.test()\n",
    "# logger.info(ranks_count)\n",
    "\n",
    "use_embeddings_pickle = False\n",
    "embeddings_pickle_name = 'doc2vec_embeddings.pkl'\n",
    "\n",
    "if use_embeddings_pickle and os.path.isfile(embeddings_pickle_name):\n",
    "    logger.info('Loading pickled embeddings')\n",
    "    with open(embeddings_pickle_name, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "else:\n",
    "    logger.info('Obtaining embeddings')\n",
    "    embeddings = d2v.generate_embeddings(\n",
    "        training_pos_files=training_pos_files,\n",
    "        training_neg_files=training_neg_files,\n",
    "        testing_pos_files=testing_pos_files,\n",
    "        testing_neg_files=testing_neg_files\n",
    "    )\n",
    "\n",
    "    with open(embeddings_pickle_name, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    *embeddings[TRAINING_DATA][SENTIMENTS.pos.review_label],\n",
    "    *embeddings[TRAINING_DATA][SENTIMENTS.neg.review_label]\n",
    "])\n",
    "X_test = np.array([\n",
    "    *embeddings[TESTING_DATA][SENTIMENTS.pos.review_label],\n",
    "    *embeddings[TESTING_DATA][SENTIMENTS.neg.review_label]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, pipeline_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# SVC\n",
    "#####\n",
    "logger.info('Training SVM with embeddings')\n",
    "svm = SVMSklearn()\n",
    "svm.train(X_train, y_train)\n",
    "\n",
    "logger.info('Testing SVM with embeddings')\n",
    "svm.cross_validate(X_train, y_train, folds=10)\n",
    "svm.test(X_train, y_train)\n",
    "svm.test(X_test, y_test)\n",
    "\n",
    "logger.info('Training SVM with pipeline embeddings')\n",
    "svm = SVMSklearn()\n",
    "svm.train(pipeline_X_train, y_train)\n",
    "\n",
    "logger.info('Testing SVM with pipeline embeddings')\n",
    "svm.cross_validate(pipeline_X_train, y_train, folds=10)\n",
    "svm.test(pipeline_X_train, y_train)\n",
    "svm.test(pipeline_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps = 5000\n",
    "X = np.vstack((X_train[:samps], X_train[-samps:], X_test[:samps], X_test[-samps:]))\n",
    "y = np.hstack((y_train[:samps], y_train[-samps:], y_test[:samps], y_test[-samps:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# PCA\n",
    "#####\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# T-SNE\n",
    "#######\n",
    "logger.info('Training T-SNE model')\n",
    "\n",
    "# tsne = TSNESklearn()\n",
    "# tsne_results = tsne.fit_transform(X)\n",
    "\n",
    "tsne = TSNE(early_exaggeration=12.0, n_components=2, learning_rate='auto', init='random', verbose=3)\n",
    "tsne_results = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame({\n",
    "    'tsne-3d-one': tsne_results[:,0],\n",
    "    'tsne-3d-two': tsne_results[:,1],\n",
    "    # 'tsne-3d-three': tsne_results[:,2],\n",
    "    'y': y,\n",
    "    'size': 2*np.ones(len(y))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(tsne_df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tsne_df, x='tsne-3d-one', y='tsne-3d-two', color='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(tsne_df, x='tsne-3d-one', y='tsne-3d-two', z='tsne-3d-three', color='y', size='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    # palette=sns.color_palette(\"hls\", 10),\n",
    "    data=tsne_df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0fa47442b815d01bf03fa5e1a4dae12be90d6f5e4d1707ce225297341a1eb48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('mlmi13': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
