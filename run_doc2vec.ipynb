{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "# import tensorflow as tf\n",
    "\n",
    "from Doc2Vec import Doc2Vec, GensimSVMSklearn\n",
    "from Constants import SENTIMENTS, TRAINING_DATA, TESTING_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Original review data\n",
    "#####################\n",
    "# base_dir = os.path.join('data', 'reviews')\n",
    "# pos_dir = os.path.join(base_dir, 'POS')\n",
    "# neg_dir = os.path.join(base_dir, 'NEG')\n",
    "\n",
    "# training_pos_files = glob(os.path.join(pos_dir, 'cv[0-8]*.txt'))\n",
    "# training_neg_files = glob(os.path.join(neg_dir, 'cv[0-8]*.txt'))\n",
    "\n",
    "# testing_pos_files = glob(os.path.join(pos_dir, 'cv9*.txt'))\n",
    "# testing_neg_files = glob(os.path.join(neg_dir, 'cv9*.txt'))\n",
    "\n",
    "# d2v_training_files = [\n",
    "#     *training_pos_files,\n",
    "#     *training_neg_files,\n",
    "#     *testing_pos_files,\n",
    "#     *testing_neg_files\n",
    "# ]\n",
    "# d2v_testing_files = []\n",
    "\n",
    "# gensim_sklearn = GensimSVMSklearn(\n",
    "#     d2v_training_files=d2v_training_files,\n",
    "#     d2v_epochs=100,\n",
    "#     d2v_infer_epochs=50,\n",
    "#     d2v_min_count=5,\n",
    "#     d2v_vector_size=50,\n",
    "#     d2v_window=5,\n",
    "#     d2v_dm=0,\n",
    "#     d2v_dm_concat=0,\n",
    "#     d2v_dbow_words=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMDB data\n",
    "###########\n",
    "base_dir = 'imdb'\n",
    "pos_dir = 'pos'\n",
    "neg_dir = 'neg'\n",
    "unsup_dir = 'unsup'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "train_pos_dir = os.path.join(train_dir, pos_dir)\n",
    "train_neg_dir = os.path.join(train_dir, neg_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'train')\n",
    "test_pos_dir = os.path.join(test_dir, pos_dir)\n",
    "test_neg_dir = os.path.join(test_dir, neg_dir)\n",
    "\n",
    "training_pos_files = glob(os.path.join(train_pos_dir, '*.txt'))\n",
    "training_neg_files = glob(os.path.join(train_neg_dir, '*.txt'))\n",
    "\n",
    "testing_pos_files = glob(os.path.join(test_pos_dir, '*.txt'))\n",
    "testing_neg_files = glob(os.path.join(test_neg_dir, '*.txt'))\n",
    "\n",
    "unsup_files = glob(os.path.join(base_dir, train_dir, unsup_dir, '*.txt'))\n",
    "\n",
    "d2v_training_files = [\n",
    "    *training_pos_files,\n",
    "    *training_neg_files,\n",
    "    *testing_pos_files,\n",
    "    *testing_neg_files,\n",
    "    *unsup_files\n",
    "]\n",
    "d2v_testing_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([\n",
    "    *[SENTIMENTS.pos.review_label]*len(training_pos_files),\n",
    "    *[SENTIMENTS.neg.review_label]*len(training_neg_files)\n",
    "])\n",
    "y_test = np.array([\n",
    "    *[SENTIMENTS.pos.review_label]*len(testing_pos_files),\n",
    "    *[SENTIMENTS.neg.review_label]*len(testing_neg_files)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Pipeline with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn = GensimSVMSklearn(\n",
    "#     d2v_training_files=d2v_training_files,\n",
    "#     d2v_epochs=100,\n",
    "#     d2v_infer_epochs=50,\n",
    "#     d2v_min_count=5,\n",
    "#     d2v_vector_size=50,\n",
    "#     d2v_window=5,\n",
    "#     d2v_dm=0,\n",
    "#     d2v_dm_concat=0,\n",
    "#     d2v_dbow_words=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('d2v_imdb.pkl', 'wb') as f:\n",
    "#     gensim_sklearn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.train([*training_pos_files, *training_neg_files], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.test([*training_pos_files, *training_neg_files], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.test([*testing_pos_files, *testing_neg_files], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.cross_validate([*training_pos_files, *training_neg_files], y_train, folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_params = {\n",
    "#     'doc2vec__epochs': (100,),\n",
    "#     'doc2vec__infer_epochs': (50,),\n",
    "#     'doc2vec__vector_size': (50,), \n",
    "#     'doc2vec__dm': (0,),\n",
    "#     'doc2vec__dm_concat': (0,),\n",
    "#     'doc2vec__dbow_words': (1,),\n",
    "#     'doc2vec__window': (5,10,15,20),\n",
    "#     'doc2vec__min_count': (5,)\n",
    "# }\n",
    "        \n",
    "# gensim_sklearn.grid_search([*training_pos_files, *training_neg_files], y_train, gs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(gensim_sklearn.gs.cv_results_).to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Elements to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('.pkl', 'wb') as f:\n",
    "#     pickle.dump(gensim_sklearn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.pipeline.named_steps['doc2vec'].model.save('doc2vec_model.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_pos_embeddings = gensim_sklearn.pipeline.named_steps['doc2vec'].transform(training_pos_files)\n",
    "# training_neg_embeddings = gensim_sklearn.pipeline.named_steps['doc2vec'].transform(training_neg_files)\n",
    "# testing_pos_embeddings = gensim_sklearn.pipeline.named_steps['doc2vec'].transform(testing_pos_files)\n",
    "# testing_neg_embeddings = gensim_sklearn.pipeline.named_steps['doc2vec'].transform(testing_neg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sg = Doc2Vec.load(os.path.join('wiki_sg', 'word2vec.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########\n",
    "# # Doc2Vec\n",
    "# #########\n",
    "\n",
    "# use_d2v_pickle = False\n",
    "# d2v_pickle_name = 'doc2vec_model.pkl'\n",
    "\n",
    "# if use_d2v_pickle and os.path.isfile(d2v_pickle_name):\n",
    "#     logger.info('Loading pickled d2v model')\n",
    "#     with open(d2v_pickle_name, 'rb') as f:\n",
    "#         d2v = pickle.load(f)\n",
    "# else:\n",
    "#     d2v = Doc2Vec(vector_size=50, epochs=40)\n",
    "\n",
    "#     logger.info('Loading data')\n",
    "#     d2v.load_data(training_files=d2v_training_files, testing_files=d2v_testing_files)\n",
    "\n",
    "#     logger.info('Training doc2vec')\n",
    "#     d2v.train()\n",
    "\n",
    "#     with open(d2v_pickle_name, 'wb') as f:\n",
    "#         pickle.dump(d2v, f)\n",
    "\n",
    "# # logger.info('Testing doc2vec on the training data')\n",
    "# # ranks_count, errors = d2v.test()\n",
    "# # logger.info(ranks_count)\n",
    "\n",
    "# use_embeddings_pickle = False\n",
    "# embeddings_pickle_name = 'doc2vec_embeddings.pkl'\n",
    "\n",
    "# if use_embeddings_pickle and os.path.isfile(embeddings_pickle_name):\n",
    "#     logger.info('Loading pickled embeddings')\n",
    "#     with open(embeddings_pickle_name, 'rb') as f:\n",
    "#         embeddings = pickle.load(f)\n",
    "# else:\n",
    "#     logger.info('Obtaining embeddings')\n",
    "#     embeddings = d2v.generate_embeddings(\n",
    "#         training_pos_files=training_pos_files,\n",
    "#         training_neg_files=training_neg_files,\n",
    "#         testing_pos_files=testing_pos_files,\n",
    "#         testing_neg_files=testing_neg_files\n",
    "#     )\n",
    "\n",
    "#     with open(embeddings_pickle_name, 'wb') as f:\n",
    "#         pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array([\n",
    "#     *embeddings[TRAINING_DATA][SENTIMENTS.pos.review_label],\n",
    "#     *embeddings[TRAINING_DATA][SENTIMENTS.neg.review_label]\n",
    "# ])\n",
    "# X_test = np.array([\n",
    "#     *embeddings[TESTING_DATA][SENTIMENTS.pos.review_label],\n",
    "#     *embeddings[TESTING_DATA][SENTIMENTS.neg.review_label]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####\n",
    "# # SVC\n",
    "# #####\n",
    "# logger.info('Training SVM with embeddings')\n",
    "# svm = SVC()\n",
    "# svm.train(X_train, y_train)\n",
    "\n",
    "# logger.info('Testing SVM with embeddings')\n",
    "# svm.cross_validate(X_train, y_train, folds=10)\n",
    "# svm.test(X_train, y_train)\n",
    "# svm.test(X_test, y_test)\n",
    "\n",
    "# logger.info('Training SVM with pipeline embeddings')\n",
    "# svm = SVC()\n",
    "# svm.train(pipeline_X_train, y_train)\n",
    "\n",
    "# logger.info('Testing SVM with pipeline embeddings')\n",
    "# svm.cross_validate(pipeline_X_train, y_train, folds=10)\n",
    "# svm.test(pipeline_X_train, y_train)\n",
    "# svm.test(pipeline_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_dimensions(model):\n",
    "#     num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "#     # extract the words & their vectors, as numpy arrays\n",
    "#     vectors = np.asarray(model.wv.vectors)\n",
    "#     labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "#     # reduce using t-SNE\n",
    "#     tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "#     vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "#     x_vals = [v[0] for v in vectors]\n",
    "#     y_vals = [v[1] for v in vectors]\n",
    "#     return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "# x_vals, y_vals, labels = reduce_dimensions(gensim_sklearn.pipeline.named_steps['doc2vec'].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_with_plotly(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_matplotlib(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_word_embeddings = gensim_sklearn.pipeline.named_steps['doc2vec'].model[gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv.save_word2vec_format('gensim_word_embeddings.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m gensim.scripts.word2vec2tensor -i gensim_word_embeddings.gensim -o gensim_word_embeddings.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_dict = {}\n",
    "with open('data/sent_lexicon', 'r') as f:\n",
    "    for line in f:\n",
    "        word = line.split()[2].split(\"=\")[1]\n",
    "        polarity = line.split()[5].split(\"=\")[1]\n",
    "        magnitude = line.split()[0].split(\"=\")[1]\n",
    "        lexicon_dict[word] = [magnitude, polarity]\n",
    "lexicon_pos_words = [w for w,t in lexicon_dict.items() if t[1] == 'positive' and w in gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv]\n",
    "lexicon_neg_words = [w for w,t in lexicon_dict.items() if t[1] == 'negative' and w in gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv]\n",
    "len(lexicon_pos_words), len(lexicon_neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_pos_word_embeddings = np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in lexicon_pos_words])\n",
    "lexicon_neg_word_embeddings = np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in lexicon_neg_words])\n",
    "lexicon_pos_word_embeddings.shape, lexicon_neg_word_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = gensim_word_embeddings\n",
    "# pca_embeddings = np.vstack((lexicon_pos_word_embeddings, lexicon_neg_word_embeddings))\n",
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_embeds_2d = pca_2.fit(pca_embeddings)\n",
    "\n",
    "pca_3 = PCA(n_components=3)\n",
    "pca_embeds_3d = pca_3.fit(pca_embeddings)\n",
    "\n",
    "pca_embeds_2d.explained_variance_ratio_, pca_embeds_3d.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_pos_words_pca = pca_embeds_2d.transform(lexicon_pos_word_embeddings)\n",
    "lexicon_neg_words_pca = pca_embeds_2d.transform(lexicon_neg_word_embeddings)\n",
    "lexicon_pos_words_pca.shape, lexicon_neg_words_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed_tuple in [('pos', lexicon_pos_words_pca), ('neg', lexicon_neg_words_pca)]:\n",
    "    label, embeds = embed_tuple\n",
    "    plt.scatter(embeds[:,0], embeds[:,1], marker='x', label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_pos_words_pca_3d = pca_embeds_3d.transform(np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in lexicon_pos_words if w in gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv]))\n",
    "lexicon_neg_words_pca_3d = pca_embeds_3d.transform(np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in lexicon_neg_words if w in gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv]))\n",
    "\n",
    "lexicon_pos_words_pca_df = pd.DataFrame(lexicon_pos_words_pca_3d)\n",
    "lexicon_pos_words_pca_df['sentiment'] = 'pos'\n",
    "\n",
    "lexicon_neg_words_pca_df = pd.DataFrame(lexicon_neg_words_pca_3d)\n",
    "lexicon_neg_words_pca_df['sentiment'] = 'neg'\n",
    "\n",
    "lexicon_pca_df = pd.concat((lexicon_pos_words_pca_df, lexicon_neg_words_pca_df))\n",
    "lexicon_pca_df['size'] = 0.8\n",
    "\n",
    "px.scatter_3d(lexicon_pca_df, x=0, y=1, z=2, color='sentiment', size='size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_review_ind = 123\n",
    "visualisation_review_words = [w for w in gensim_sklearn.pipeline.named_steps['doc2vec'].train_corpus[visualisation_review_ind].words if w in gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv]\n",
    "visualisation_review_word_embeddings = np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in visualisation_review_words])\n",
    "visualisation_review_embedding = gensim_sklearn.pipeline.named_steps['doc2vec'].transform([os.path.join(train_pos_dir,gensim_sklearn.pipeline.named_steps['doc2vec'].train_corpus[visualisation_review_ind].tags[0])])\n",
    "visualisation_words = [*visualisation_review_words, 'review']\n",
    "visualisation_sentiment = [lexicon_dict.get(w, [None, None])[1] for w in visualisation_review_words]\n",
    "visualisation_pos_inds = [i for i,x in enumerate(visualisation_sentiment) if x == 'positive']\n",
    "visualisation_neg_inds = [i for i,x in enumerate(visualisation_sentiment) if x == 'negative']\n",
    "visualisation_sentiment_inds = [*visualisation_pos_inds, *visualisation_neg_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_words_pca = pca_2.transform(visualisation_review_word_embeddings)\n",
    "visualisation_review_pca = pca_2.transform(visualisation_review_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lookup_tuple in (('pos', visualisation_pos_inds), ('neg', visualisation_neg_inds)):\n",
    "    label, lookup_ind = lookup_tuple\n",
    "    plt.scatter(visualisation_words_pca[lookup_ind,0], visualisation_words_pca[lookup_ind,1], marker='x', label=label)\n",
    "plt.scatter(visualisation_review_pca[-1,0], visualisation_review_pca[-1,1], marker='x', label='review')\n",
    "plt.legend()\n",
    "for word_ind in visualisation_sentiment_inds:\n",
    "    plt.text(visualisation_words_pca[word_ind,0]+0.02, visualisation_words_pca[word_ind,1]+0.02, visualisation_words[word_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pang et al words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pang_positive_words = 'love wonderful best great superb still beautiful'.split()\n",
    "pang_negative_words = 'bad worst stupid waste boring terrible awful'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pang_positive_word_embeddings = np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in pang_positive_words])\n",
    "pang_negative_word_embeddings = np.array([gensim_sklearn.pipeline.named_steps['doc2vec'].model.wv[w] for w in pang_negative_words])\n",
    "pang_positive_word_embeddings.shape, pang_negative_word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pang_positive_word_pca = pca_2.transform(pang_positive_word_embeddings)\n",
    "pang_negative_word_pca = pca_2.transform(pang_negative_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc_ind = 10\n",
    "neg_doc_ind = 10\n",
    "pos_doc_pca = pca_2.transform(training_pos_embeddings[pos_doc_ind,:][None,:])[0]\n",
    "neg_doc_pca = pca_2.transform(training_neg_embeddings[neg_doc_ind,:][None,:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pang_positive_word_pca[:,0], pang_positive_word_pca[:,1], marker='x', label='pos')\n",
    "plt.scatter(pang_negative_word_pca[:,0], pang_negative_word_pca[:,1], marker='x', label='neg')\n",
    "plt.scatter(pos_doc_pca[0], pos_doc_pca[1], marker='x', label='pos_doc')\n",
    "plt.scatter(neg_doc_pca[0], neg_doc_pca[1], marker='x', label='neg_doc')\n",
    "plt.legend()\n",
    "for word, (x,y) in zip([*pang_positive_words, *pang_negative_words], np.vstack((pang_positive_word_pca, pang_negative_word_pca))):\n",
    "    plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Training T-SNE model')\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=3, n_jobs=-1)\n",
    "# tsne = TSNE(n_components=2, early_exaggeration=12.0, learning_rate='auto', init='random', verbose=3)\n",
    "\n",
    "tsne_results = tsne.fit_transform(gensim_word_embeddings)\n",
    "\n",
    "# Dataframe construction\n",
    "tsne_df = pd.DataFrame({\n",
    "    'tsne-one': tsne_results[:,0],\n",
    "    'tsne-two': tsne_results[:,1],\n",
    "    # 'tsne-three': tsne_results[:,2],\n",
    "    'word': tsne_words\n",
    "})\n",
    "tsne_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_lexicon_df = tsne_df.copy().loc[tsne_df['word'].isin(lexicon_dict)]\n",
    "tsne_lexicon_df['lexicon'] = tsne_lexicon_df['word'].map(lexicon_dict)\n",
    "tsne_lexicon_df[['magnitude','sentiment']] = pd.DataFrame(tsne_lexicon_df['lexicon'].tolist(), index=tsne_lexicon_df.index)\n",
    "tsne_lexicon_df = pd.concat((tsne_lexicon_df, tsne_df.copy().loc[tsne_df['word'] == 'review']))\n",
    "tsne_lexicon_df.loc[tsne_lexicon_df['word'] == 'review', 'sentiment'] = 'review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tsne_lexicon_df , x='tsne-one', y='tsne-two', color='sentiment', hover_name='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_pang_df = tsne_df.copy()[tsne_df['word'].isin([*pang_positive_words, *pang_negative_words])]\n",
    "tsne_pang_df.loc[tsne_pang_df['word'].isin(pang_positive_words), 'sentiment'] = 'pos'\n",
    "tsne_pang_df.loc[tsne_pang_df['word'].isin(pang_negative_words), 'sentiment'] = 'neg'\n",
    "tsne_pang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tsne_pang_df , x='tsne-one', y='tsne-two', color='sentiment', hover_name='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px.scatter_3d(tsne_df, x='tsne-3d-one', y='tsne-3d-two', z='tsne-3d-three', color='y', size='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-one\", y=\"tsne-two\",\n",
    "    hue=\"sentiment\",\n",
    "    # palette=sns.color_palette(\"hls\", 10),\n",
    "    data=tsne_pang_df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0fa47442b815d01bf03fa5e1a4dae12be90d6f5e4d1707ce225297341a1eb48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('mlmi13': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
